{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oycG0Y5MgjH-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tabulate\n",
        "from recsysNN_utils import *\n",
        "pd.set_option(\"display.precision\", 1)\n",
        "\n",
        "top10_df = pd.read_csv(\"./data/content_top10_df.csv\")\n",
        "bygenre_df = pd.read_csv(\"./data/content_bygenre_df.csv\")\n",
        "\n",
        "# Load Data, set configuration variables\n",
        "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
        "\n",
        "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
        "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
        "uvs = 3  # user genre vector start\n",
        "ivs = 3  # item genre vector start\n",
        "u_s = 3  # start of columns to use in training, user\n",
        "i_s = 1  # start of columns to use in training, items\n",
        "print(f\"Number of training vectors: {len(item_train)}\")\n",
        "\n",
        "pprint_train(user_train, user_features, uvs,  u_s, maxcount=5)\n",
        "\n",
        "pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)\n",
        "\n",
        "print(f\"y_train[:5]: {y_train[:5]}\")\n",
        "\n",
        "# scale training data\n",
        "item_train_unscaled = item_train\n",
        "user_train_unscaled = user_train\n",
        "y_train_unscaled    = y_train\n",
        "\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
        "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))\n",
        "\n",
        "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
        "print(f\"movie/item training data shape: {item_train.shape}\")\n",
        "print(f\"movie/item test data shape: {item_test.shape}\")\n",
        "\n",
        "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)\n",
        "\n",
        "num_outputs = 32\n",
        "tf.random.set_seed(1)\n",
        "user_NN = tf.keras.models.Sequential([ \n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs),\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([  \n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs),\n",
        "])\n",
        "\n",
        "# create the user input and point to the base network\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "# create the item input and point to the base network\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "# compute the dot product of the two vectors vu and vm\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "# specify the inputs and output of the model\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Public tests\n",
        "from public_tests import *\n",
        "test_tower(user_NN)\n",
        "test_tower(item_NN)\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=cost_fn)\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)\n",
        "\n",
        "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)\n",
        "\n",
        "new_user_id = 5000\n",
        "new_rating_ave = 0.0\n",
        "new_action = 0.0\n",
        "new_adventure = 5.0\n",
        "new_animation = 0.0\n",
        "new_childrens = 0.0\n",
        "new_comedy = 0.0\n",
        "new_crime = 0.0\n",
        "new_documentary = 0.0\n",
        "new_drama = 0.0\n",
        "new_fantasy = 5.0\n",
        "new_horror = 0.0\n",
        "new_mystery = 0.0\n",
        "new_romance = 0.0\n",
        "new_scifi = 0.0\n",
        "new_thriller = 0.0\n",
        "new_rating_count = 3\n",
        "\n",
        "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
        "                      new_action, new_adventure, new_animation, new_childrens,\n",
        "                      new_comedy, new_crime, new_documentary,\n",
        "                      new_drama, new_fantasy, new_horror, new_mystery,\n",
        "                      new_romance, new_scifi, new_thriller]])\n",
        "\n",
        "# generate and replicate the user vector to match the number movies in the data set.\n",
        "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "\n",
        "print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)\n",
        "\n",
        "uid = 2 \n",
        "# form a set of user vectors. This is the same vector, transformed and repeated.\n",
        "user_vecs, y_vecs = get_user_vecs(uid, user_train_unscaled, item_vecs, user_to_genre)\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "sorted_user  = user_vecs[sorted_index]\n",
        "sorted_y     = y_vecs[sorted_index]\n",
        "\n",
        "#print sorted predictions for movies rated by the user\n",
        "print_existing_user(sorted_ypu, sorted_y.reshape(-1,1), sorted_user, sorted_items, ivs, uvs, movie_dict, maxcount = 50)\n",
        "\n",
        "def sq_dist(a,b):   \n",
        "    d = sum(np.square(a-b)) \n",
        "    return d\n",
        "\n",
        "a1 = np.array([1.0, 2.0, 3.0]); b1 = np.array([1.0, 2.0, 3.0])\n",
        "a2 = np.array([1.1, 2.1, 3.1]); b2 = np.array([1.0, 2.0, 3.0])\n",
        "a3 = np.array([0, 1, 0]);       b3 = np.array([1, 0, 0])\n",
        "print(f\"squared distance between a1 and b1: {sq_dist(a1, b1):0.3f}\")\n",
        "print(f\"squared distance between a2 and b2: {sq_dist(a2, b2):0.3f}\")\n",
        "print(f\"squared distance between a3 and b3: {sq_dist(a3, b3):0.3f}\")\n",
        "\n",
        "# Public tests\n",
        "test_sq_dist(sq_dist)\n",
        "\n",
        "input_item_m = tf.keras.layers.Input(shape=(num_item_features))    # input layer\n",
        "vm_m = item_NN(input_item_m)                                       # use the trained item_NN\n",
        "vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        # incorporate normalization as was done in the original model\n",
        "model_m = tf.keras.Model(input_item_m, vm_m)                                \n",
        "model_m.summary()\n",
        "\n",
        "scaled_item_vecs = scalerItem.transform(item_vecs)\n",
        "vms = model_m.predict(scaled_item_vecs[:,i_s:])\n",
        "print(f\"size of all predicted movie feature vectors: {vms.shape}\")\n",
        "\n",
        "count = 50  # number of movies to display\n",
        "dim = len(vms)\n",
        "dist = np.zeros((dim,dim))\n",
        "\n",
        "for i in range(dim):\n",
        "    for j in range(dim):\n",
        "        dist[i,j] = sq_dist(vms[i, :], vms[j, :])\n",
        "        \n",
        "m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal\n",
        "\n",
        "disp = [[\"movie1\", \"genres\", \"movie2\", \"genres\"]]\n",
        "for i in range(count):\n",
        "    min_idx = np.argmin(m_dist[i])\n",
        "    movie1_id = int(item_vecs[i,0])\n",
        "    movie2_id = int(item_vecs[min_idx,0])\n",
        "    disp.append( [movie_dict[movie1_id]['title'], movie_dict[movie1_id]['genres'],\n",
        "                  movie_dict[movie2_id]['title'], movie_dict[movie1_id]['genres']]\n",
        "               )\n",
        "table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
        "table"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ij9tO6llMlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjustment Code"
      ],
      "metadata": {
        "id": "e3xsn_ZxlNT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tabulate\n",
        "from recsysNN_utils import *\n",
        "pd.set_option(\"display.precision\", 1)"
      ],
      "metadata": {
        "id": "zXvP8Y2VlXC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top10_df = pd.read_csv(\"./data/content_top10_df.csv\")\n",
        "bygenre_df = pd.read_csv(\"./data/content_bygenre_df.csv\")"
      ],
      "metadata": {
        "id": "mWlaD1zelXGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data, set configuration variables\n",
        "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
        "\n",
        "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
        "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
        "uvs = 3  # user genre vector start\n",
        "ivs = 3  # item genre vector start\n",
        "u_s = 3  # start of columns to use in training, user\n",
        "i_s = 1  # start of columns to use in training, items\n",
        "print(f\"Number of training vectors: {len(item_train)}\")\n",
        "\n",
        "pprint_train(user_train, user_features, uvs,  u_s, maxcount=5)\n",
        "\n",
        "pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)\n",
        "\n",
        "print(f\"y_train[:5]: {y_train[:5]}\")"
      ],
      "metadata": {
        "id": "BpsCf_d0lYna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale training data\n",
        "item_train_unscaled = item_train\n",
        "user_train_unscaled = user_train\n",
        "y_train_unscaled    = y_train\n",
        "\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
        "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))"
      ],
      "metadata": {
        "id": "3et0XY8_lYqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
        "print(f\"movie/item training data shape: {item_train.shape}\")\n",
        "print(f\"movie/item test data shape: {item_test.shape}\")"
      ],
      "metadata": {
        "id": "amaXCXZYlY09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
      ],
      "metadata": {
        "id": "qHO5w9b5lkVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_outputs = 32\n",
        "tf.random.set_seed(1)\n",
        "user_NN = tf.keras.models.Sequential([ \n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs),\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([  \n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs),\n",
        "])"
      ],
      "metadata": {
        "id": "EaSDsG7DlkX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the user input and point to the base network\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "# create the item input and point to the base network\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "# compute the dot product of the two vectors vu and vm\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "# specify the inputs and output of the model\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IGFf4CNglkaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Public tests\n",
        "from public_tests import *\n",
        "test_tower(user_NN)\n",
        "test_tower(item_NN)\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=opt, loss=cost_fn)\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)\n",
        "\n",
        "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
      ],
      "metadata": {
        "id": "4KdVNPh3lxkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_calories=2000\n",
        "max_daily_fat=100\n",
        "max_daily_Saturatedfat=13\n",
        "max_daily_Cholesterol=300\n",
        "max_daily_Sodium=2300\n",
        "max_daily_Carbohydrate=325\n",
        "max_daily_Fiber=40\n",
        "max_daily_Sugar=40\n",
        "max_daily_Protein=200\n",
        "user_vec=[max_calories,\n",
        "          max_daily_fat,\n",
        "          max_daily_Saturatedfat,\n",
        "          max_daily_Cholesterol,\n",
        "          max_daily_Sodium,\n",
        "          max_daily_Carbohydrate,\n",
        "          max_daily_Fiber,\n",
        "          max_daily_Sugar,\n",
        "          max_daily_Protein]"
      ],
      "metadata": {
        "id": "kWkp2Phmlxmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and replicate the user vector to match the number movies in the data set.\n",
        "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])"
      ],
      "metadata": {
        "id": "-hPo0-7Nlxtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "\n",
        "print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)\n",
        "\n",
        "uid = 2 \n",
        "# form a set of user vectors. This is the same vector, transformed and repeated.\n",
        "user_vecs, y_vecs = get_user_vecs(uid, user_train_unscaled, item_vecs, user_to_genre)\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)"
      ],
      "metadata": {
        "id": "HVgKJ63zmFNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "sorted_user  = user_vecs[sorted_index]\n",
        "sorted_y     = y_vecs[sorted_index]\n",
        "\n",
        "#print sorted predictions for movies rated by the user\n",
        "print_existing_user(sorted_ypu, sorted_y.reshape(-1,1), sorted_user, sorted_items, ivs, uvs, movie_dict, maxcount = 50)\n"
      ],
      "metadata": {
        "id": "4jLFP1tFmFRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sq_dist(a,b):   \n",
        "    d = sum(np.square(a-b)) \n",
        "    return d\n",
        "\n",
        "a1 = np.array([1.0, 2.0, 3.0]); b1 = np.array([1.0, 2.0, 3.0])\n",
        "a2 = np.array([1.1, 2.1, 3.1]); b2 = np.array([1.0, 2.0, 3.0])\n",
        "a3 = np.array([0, 1, 0]);       b3 = np.array([1, 0, 0])\n",
        "print(f\"squared distance between a1 and b1: {sq_dist(a1, b1):0.3f}\")\n",
        "print(f\"squared distance between a2 and b2: {sq_dist(a2, b2):0.3f}\")\n",
        "print(f\"squared distance between a3 and b3: {sq_dist(a3, b3):0.3f}\")"
      ],
      "metadata": {
        "id": "xUSUhSzrmFZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Public tests\n",
        "test_sq_dist(sq_dist)\n",
        "\n",
        "input_item_m = tf.keras.layers.Input(shape=(num_item_features))    # input layer\n",
        "vm_m = item_NN(input_item_m)                                       # use the trained item_NN\n",
        "vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        # incorporate normalization as was done in the original model\n",
        "model_m = tf.keras.Model(input_item_m, vm_m)                                \n",
        "model_m.summary()\n",
        "\n",
        "scaled_item_vecs = scalerItem.transform(item_vecs)\n",
        "vms = model_m.predict(scaled_item_vecs[:,i_s:])\n",
        "print(f\"size of all predicted movie feature vectors: {vms.shape}\")"
      ],
      "metadata": {
        "id": "dZ09y3LVmFcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 50  # number of movies to display\n",
        "dim = len(vms)\n",
        "dist = np.zeros((dim,dim))\n",
        "\n",
        "for i in range(dim):\n",
        "    for j in range(dim):\n",
        "        dist[i,j] = sq_dist(vms[i, :], vms[j, :])\n",
        "        \n",
        "m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal"
      ],
      "metadata": {
        "id": "l_vxjAvYlMpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = [[\"movie1\", \"genres\", \"movie2\", \"genres\"]]\n",
        "for i in range(count):\n",
        "    min_idx = np.argmin(m_dist[i])\n",
        "    movie1_id = int(item_vecs[i,0])\n",
        "    movie2_id = int(item_vecs[min_idx,0])\n",
        "    disp.append( [movie_dict[movie1_id]['title'], movie_dict[movie1_id]['genres'],\n",
        "                  movie_dict[movie2_id]['title'], movie_dict[movie1_id]['genres']]\n",
        "               )\n",
        "table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
        "table"
      ],
      "metadata": {
        "id": "TRGb6qcslvwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_C7aKO5klv1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Capstone/dataset.csv\")"
      ],
      "metadata": {
        "id": "XRdHf_KqheG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "YlS0k9mWhgH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "z6W0G9xthgKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data, set configuration variables\n",
        "item_train = df[['Calories', 'ProteinContent', 'FatContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'SaturatedFatContent', 'SodiumContent', 'CholesterolContent']]\n",
        "user_train = \n",
        "y_train =\n",
        "item_features =\n",
        "user_features = \n",
        "item_vecs = \n",
        "food_dict = df[['Name']]\n",
        "\n",
        "# Preprocess Data\n",
        "num_user_features = user_train.shape[1] - 1  # remove userid during training\n",
        "num_item_features = item_train.shape[1] - 2  # remove food id and calories at train time"
      ],
      "metadata": {
        "id": "KFpO883lhgMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "\n",
        "item_train, item_test, user_train, user_test, y_train, y_test = train_test_split(\n",
        "    item_train, user_train, y_train, train_size=0.80, shuffle=True, random_state=1\n",
        ")\n",
        "\n",
        "# Define Model Architecture\n",
        "num_outputs = 32\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs),\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_outputs),\n",
        "])\n",
        "\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features,))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features,))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "model.summary()\n",
        "\n",
        "# Compile and Train Model\n",
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=opt, loss=cost_fn)\n",
        "\n",
        "model.fit([user_train, item_train[:, 1:]], y_train, epochs=30)\n",
        "\n",
        "# Generate Recommendations\n",
        "new_user_id = 5000\n",
        "new_calories = 1500\n",
        "\n",
        "user_vec = np.array([[new_user_id, new_calories]])\n",
        "\n",
        "user_vecs = gen_user_vecs(user_vec, len(item_vecs))\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "y_p = model.predict([suser_vecs, sitem_vecs[:, 1:]])\n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "sorted_index = np.argsort(-y_pu, axis=0).reshape(-1).tolist()\n",
        "sorted_ypu = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]\n",
        "\n",
        "print_pred_foods(sorted_ypu, sorted_items, food_dict, maxcount=10)"
      ],
      "metadata": {
        "id": "v2vm8OX_hgPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data, set configuration variables\n",
        "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
        "\n",
        "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
        "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
        "uvs = 3  # user genre vector start\n",
        "ivs = 3  # item genre vector start\n",
        "u_s = 3  # start of columns to use in training, user\n",
        "i_s = 1  # start of columns to use in training, items\n",
        "print(f\"Number of training vectors: {len(item_train)}\")"
      ],
      "metadata": {
        "id": "5q0XMu2KhgS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA8_XukUjcjU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}