{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import shutil\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Capstone/dataset.csv\")"
      ],
      "metadata": {
        "id": "UirKpLfigxam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "y8AOzkUPg62v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_tittle_column = hub.text_embedding_column(key='tittle',\n",
        "                                                   module_spec='',\n",
        "                                                   trainable=False)\n",
        "\n",
        "content_id_column = tf.feature_column.categorical_column_with_hash_bucket(key='content_id',\n",
        "                                                                          hash_bucket_size = len(content_ids_list)+1)\n",
        "embedded_content_column = tf.feature_column.embedding_column(categorical_column = content_id_column,\n",
        "                                                             dimension = 10)\n",
        "\n",
        "author_column = tf.feature_column.categorical_column_with_hash_bucket(key='author',\n",
        "                                                                          hash_bucket_size = len(authors_list)+1)\n",
        "embedded_author_column = tf.feature_column.embedding_column(categorical_column = content_id_column,\n",
        "                                                             dimension = 3)\n",
        "\n",
        "category_column_categorical = tf.feature_column.categorical_column_with_vocabulary_list(key='category',\n",
        "                                                                                        vocabulary_list=categories_list,\n",
        "                                                                                        num_oov_bucket = 1)\n",
        "\n",
        "category_column = tf.feature_column.indicator_column(category_column_categorical)\n",
        "\n",
        "months_since_epoch_boundaries = list(range(400,700,20))\n",
        "months_since_epoch_column = tf.feature_column.numeric.column(key = 'months_since_epoch')\n",
        "months_since_epoch_bucketized = tf.feature_column.bucketized_column(source_column = months_since_epoch_column,\n",
        "                                                                    boundaries = months_since_epoch_boudaries)\n",
        "\n",
        "crossed_month_since_category_column = tf.feature_column.indicator_column(tf.feature_column.crossed_column(keys = [categorical_column_categorical, months_since_epoch_bucketized],\n",
        "                                                                       hash_bucket_sized = len(months_since_epoch_boundaries)*(len(categories_list+1))))\n",
        "\n",
        "feature_columns = [embedded_content_column,\n",
        "                   embedded_author_column,\n",
        "                   category_column,\n",
        "                   embedded_title_column,\n",
        "                   crossed_month_since_category_colum]"
      ],
      "metadata": {
        "id": "EARToSA8g68u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_defaults = [['Unknowwn'], ['Unknown'], ['Unknown'], ['Unknown'], [mean_months_since_epoch], ['Unknown']]\n",
        "column_keys = ['visitor_id', 'content_id', 'category', 'title', 'author', 'months_since_epoch', 'next_content_id']\n",
        "label_key = 'next_content_id'\n",
        "\n",
        "def read_dataset(filename, node, batch_size = 512):\n",
        "  def _input_fn():\n",
        "    def decode_csv(value_column):\n",
        "      columns = tf.decode_csv(value_column.record_defaults = record_defaults)\n",
        "      features = dict(zip(column_keys, columns))\n",
        "      label = features.pop(label_key)\n",
        "      return features, label\n",
        "\n",
        "    file_list = tf.gfile.Glob(filename)\n",
        "\n",
        "    dataset = tf.data.TextLineDataset(filelist).map(decode_csv)\n",
        "\n",
        "    if node == tf.estimator.Nodekeys.TRAIN:\n",
        "      num_epoch = None\n",
        "      dataset = dataset.shuffle(buffer_size = 10 = batch_size)\n",
        "    else:\n",
        "      num_epoch = 1\n",
        "    \n",
        "    dataset = dataset.repeat(num_epoch).batch(batch_size)\n",
        "    return dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "  return _input_fn\n"
      ],
      "metadata": {
        "id": "4afBY2Wxg9fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, node, params):\n",
        "  net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
        "  for units in params['hidden_units']:\n",
        "    net = tf.layers.dense(net, units=units, activation = 'relu')\n",
        "\n",
        "  logits = tf.layer.dense(net, param['n_classes'], activation = None)\n",
        "\n",
        "  prediction_classes = tf.argmax(logits, 1)\n",
        "  from tensorflow.python.lib.io import file_io\n",
        "\n",
        "  with file_io.FileIO('content_ids.txt', node='r') as ifp:\n",
        "    content = tf.constant([x.rstrip() for x in ifp])\n",
        "  predicted_class_names = tf.gather(content, predicted_classes)\n",
        "  if node == tf.estimator.NodeKeys.PEEDICT:\n",
        "    predictions = (\n",
        "        'class_ids': prediction_classes[:, tf.newaxis],\n",
        "        'class_names': prediction_class_names[:, tf.newaxis],\n",
        "        'probabilities': tf.nn.softmax(logits),\n",
        "        'logits': logits,\n",
        "    )\n",
        "    return tf.EstimatorSpec(node, predictions=predictions)\n",
        "  table = tf.contrib.lookup.index_table_from_file(vocabulary_file='content_ids.txt')\n",
        "  labels = table.lookup(labels)\n",
        "\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "  accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                 predictions=predicted_classes,\n",
        "                                 name='acc_op')\n",
        "  top_10_accuracy = tf.metrics.mean(tf.nn.in_top_k(predictions=logits, targets=labels, k=10))\n",
        "\n",
        "  metrics = (\n",
        "      'accuracy': accuracy,\n",
        "      'top_10_accuracy': top_10_accuracy\n",
        "  )\n",
        "\n",
        "  tf.summary.scalar('accuracy',accuracy[1])\n",
        "  tf.summary.scalar('top_10_accuracy', top_10_accuracy[1])\n",
        "\n",
        "  if node == tf.estimator.NodeKeys.EVAL:\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        node, loss=loss, eval_metrics_ops=metrics)\n",
        "  \n",
        "  assert node == tf.estimator.NodeKeys.TRAIN\n",
        "\n",
        "  optimizer = tf.trainAdagradOptimizer(learning_rate=0.1)\n",
        "  trains_op = optimizer.minimize(loss, global_step= tf.train.get_global_step())\n",
        "  return tf.estimator.EstimatorSpec(node, loss=loss, train_op=train_op)"
      ],
      "metadata": {
        "id": "4Kv1T04Ng9wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outdir = 'content_based_model_trained'\n",
        "shutil.retree(outdir, ignore_errors = True)\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    model_dir=outdir,\n",
        "    params=(\n",
        "        'feature_colums':feature_columns,\n",
        "        'hidden_units': [200,100,50],\n",
        "        'n_classes': len(content_ids_list)\n",
        "    ))\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn = read.dataset('training_set.csv', tf.estimator.NodeKeys.TRAIN),\n",
        "    max_steps = 2000)\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "    input_fn = read_dataset('test_set.csv', tf.estimator.NodeKeys.EVAL),\n",
        "    steps = None,\n",
        "    start_delay_secs = 30,\n",
        "    throttle_secs = 60)\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "metadata": {
        "id": "k91GFkvFg91T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kp-08RcFgyQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusment Code"
      ],
      "metadata": {
        "id": "jAUjBtZZhCwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import shutil"
      ],
      "metadata": {
        "id": "-WVy4S6vfyRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membaca dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Capstone/dataset.csv\")"
      ],
      "metadata": {
        "id": "0t73dY84fyoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menginisialisasi kolom fitur\n",
        "embedded_title_column = hub.text_embedding_column(key='Name',\n",
        "                                                  module_spec='')\n",
        "\n",
        "calories_column = tf.feature_column.numeric_column(key='Calories')\n",
        "\n",
        "feature_columns = [embedded_title_column, calories_column]"
      ],
      "metadata": {
        "id": "drTGzi4Hf3uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menentukan label key (rekomendasi makanan)\n",
        "label_key = 'food_recommendation'"
      ],
      "metadata": {
        "id": "dvCwZJx7f3x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengatur default values dan keys untuk kolom-kolom dataset\n",
        "record_defaults = [['RecipeId'], [0]]\n",
        "column_keys = ['Name', 'Calories']"
      ],
      "metadata": {
        "id": "iXx78nvyf4F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk membaca dataset\n",
        "def read_dataset(filename, mode, batch_size=512):\n",
        "    def _input_fn():\n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(value_column, record_defaults=record_defaults)\n",
        "            features = dict(zip(column_keys, columns))\n",
        "            label = features.pop(label_key)\n",
        "            return features, label\n",
        "\n",
        "        dataset = tf.data.TextLineDataset(filename).map(decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
        "\n",
        "        dataset = dataset.repeat().batch(batch_size)\n",
        "        return dataset\n",
        "\n",
        "    return _input_fn"
      ],
      "metadata": {
        "id": "zNsyrt0Pfy3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outdir = 'content_based_model_trained'\n",
        "shutil.rmtree(outdir, ignore_errors=True)"
      ],
      "metadata": {
        "id": "3YEDoHsygBSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Function\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Mendefinisikan layer input menggunakan feature columns\n",
        "    input_layer = tf.feature_column.input_layer(features, params['feature_columns'])\n",
        "\n",
        "    # Mendefinisikan hidden layers\n",
        "    hidden_layer1 = tf.layers.dense(inputs=input_layer, units=200, activation=tf.nn.relu)\n",
        "    hidden_layer2 = tf.layers.dense(inputs=hidden_layer1, units=100, activation=tf.nn.relu)\n",
        "    hidden_layer3 = tf.layers.dense(inputs=hidden_layer2, units=50, activation=tf.nn.relu)\n",
        "\n",
        "    # Mendefinisikan output layer (rekomendasi makanan)\n",
        "    logits = tf.layers.dense(inputs=hidden_layer3, units=params['n_classes'])\n",
        "\n",
        "    # Menghasilkan prediksi jika mode adalah PREDICT\n",
        "    predictions = {\n",
        "        'food_recommendation': tf.argmax(logits, axis=1),\n",
        "    }\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "    # Menghitung loss\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # Menghitung accuracy\n",
        "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions['food_recommendation'])\n",
        "\n",
        "    # Konfigurasi metrik evaluasi\n",
        "    metrics = {'accuracy': accuracy}\n",
        "    tf.summary.scalar('accuracy', accuracy[1])\n",
        "\n",
        "    # Konfigurasi metode evaluasi dan training\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "\n",
        "    # Konfigurasi metode training\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
      ],
      "metadata": {
        "id": "w-u3W6lTgBVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat Estimator\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    model_dir=outdir,\n",
        "    params={\n",
        "        'feature_columns': feature_columns,\n",
        "        'hidden_units': [200, 100, 50],\n",
        "        'n_classes': len(df['food_recommendation'].unique())\n",
        "    })"
      ],
      "metadata": {
        "id": "arkTqPqogGlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spesifikasi pelatihan\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn=read_dataset('dataset.csv', tf.estimator.ModeKeys.TRAIN),\n",
        "    max_steps=2000)"
      ],
      "metadata": {
        "id": "7OSt8EeigGox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spesifikasi evaluasi\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "    input_fn=read_dataset('dataset.csv', tf.estimator.ModeKeys.EVAL),\n",
        "    steps=None,\n",
        "    start_delay_secs=30,\n",
        "    throttle_secs=60)"
      ],
      "metadata": {
        "id": "OTByZ1N-gKoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih dan mengevaluasi model\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
      ],
      "metadata": {
        "id": "GR_RKTMseC09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}